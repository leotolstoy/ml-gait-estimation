{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to train the primary gait transformer, which translates a buffer of kinematics data to a gait state. For newcomers to this repo, this is where you should start if you want to train your own Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DoK10It3TUN2",
    "outputId": "115d4998-5725-4fe6-d9c4-7f5597f5eeff"
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# tf.config.list_physical_devices('GPU')\n",
    "# tf.test.is_built_with_cuda()\n",
    "import os, sys\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import math\n",
    "from torch import nn, Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import pyarrow.parquet as pq\n",
    "import time, datetime\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "from datasets import WindowedGaitDataset, ToTensor, ExobootDataset\n",
    "from gait_transformer import GaitTransformer\n",
    "from save_best_model import SaveBestModel\n",
    "from training_utils import phase_dist, unscale_kinematics, unscale_gait_state\n",
    "from torch_training_utils import GaitLoss, EWC, enum_parameters\n",
    "\n",
    "\n",
    "import random\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "#RUN THIS ON COLAB\n",
    "ON_COLAB = False\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    drive_path = '/content/drive/MyDrive/Phase ML Data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the datafiles containing the walking data used to train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/r01_ordered_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/r01_randomized_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/dataport_ordered_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/dataport_randomized_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/gt_ordered_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/gt_randomized_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://ml_gait_estimation/r01_ordered_stairs_label_corrupt_time.csv .\n",
    "!gsutil cp -r gs://ml_gait_estimation/r01_randomized_stairs_label_corrupt_time.csv .\n",
    "!gsutil cp -r gs://ml_gait_estimation/dataport_ordered_stairs_label_corrupt_time.csv .\n",
    "!gsutil cp -r gs://ml_gait_estimation/dataport_randomized_stairs_label_corrupt_time.csv .\n",
    "!gsutil cp -r gs://ml_gait_estimation/gt_ordered_stairs_label_corrupt_time.csv .\n",
    "!gsutil cp -r gs://ml_gait_estimation/gt_randomized_stairs_label_corrupt_time.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up dataframes, Datasets, and DataLoaders to contain the kinematics and gait state labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TDj35NaeXVk"
   },
   "outputs": [],
   "source": [
    "window_size = 150 #set the number of kinematics in the buffer to the transformer\n",
    "\n",
    "#set up kinematics scaling\n",
    "meas_scale = np.array([[-69.35951035,  27.62815047],\\\n",
    "                        [-456.18013759,  401.13782617],\\\n",
    "                        [-63.71649984,  22.06632622],\\\n",
    "                        [-213.4786175,   396.93801619],\\\n",
    "                        [-35.26603985,  20.78473636],\\\n",
    "                        [-20.95456523,  14.63961137],\\\n",
    "                          [0,1]])\n",
    "\n",
    "#set up gait state scaling\n",
    "speed_scale = (0,2)\n",
    "incline_scale = (-10,10)\n",
    "stair_height_scale = (-1,1)\n",
    "\n",
    "#set up filenames for each data file\n",
    "filename_dataport_ordered = 'dataport_ordered_stairs_label_corrupt_time.csv'\n",
    "filename_dataport_randomized = 'dataport_randomized_stairs_label_corrupt_time.csv'\n",
    "\n",
    "filename_r01_ordered = 'r01_ordered_stairs_label_corrupt_time.csv'\n",
    "filename_r01_randomized = 'r01_randomized_stairs_label_corrupt_time.csv'\n",
    "\n",
    "filename_gt_ordered = 'gt_ordered_stairs_label_corrupt_time.csv'\n",
    "filename_gt_randomized = 'gt_randomized_stairs_label_corrupt_time.csv'\n",
    "\n",
    "if ON_COLAB:\n",
    "    filename_r01_ordered = drive_path+filename_r01_ordered\n",
    "    filename_r01_randomized = drive_path+filename_r01_randomized\n",
    "    filename_dataport_ordered = drive_path+filename_dataport_ordered\n",
    "    filename_dataport_randomized = drive_path+filename_dataport_randomized\n",
    "    filename_gt_ordered = drive_path+filename_gt_ordered\n",
    "    filename_gt_randomized = drive_path+filename_gt_randomized\n",
    "    \n",
    "\n",
    "DO_DECIMATION = not True\n",
    "if DO_DECIMATION:\n",
    "    gait_data_r01_randomized = pd.read_csv(filename_r01_randomized, nrows=10000)\n",
    "    gait_data_r01_ordered = pd.read_csv(filename_r01_ordered, nrows=10000)\n",
    "    gait_data_dataport_ordered = pd.read_csv(filename_dataport_ordered, nrows=10000)\n",
    "    gait_data_dataport_randomized = pd.read_csv(filename_dataport_randomized, nrows=10000)\n",
    "    gait_data_gt_ordered = pd.read_csv(filename_gt_ordered, nrows=10000)\n",
    "    gait_data_gt_randomized = pd.read_csv(filename_gt_randomized, nrows=10000)\n",
    "\n",
    "    \n",
    "else:\n",
    "    gait_data_r01_randomized = pd.read_csv(filename_r01_randomized)\n",
    "    gait_data_r01_ordered = pd.read_csv(filename_r01_ordered)\n",
    "    gait_data_dataport_ordered = pd.read_csv(filename_dataport_ordered)\n",
    "    gait_data_dataport_randomized = pd.read_csv(filename_dataport_randomized)\n",
    "    gait_data_gt_ordered = pd.read_csv(filename_gt_ordered)\n",
    "    gait_data_gt_randomized = pd.read_csv(filename_gt_randomized)\n",
    "\n",
    "\n",
    "# REMOVE RANDOM SUBJECTS FOR X-VALIDATION\n",
    "#FROM R01, remove two: AB02 and AB06\n",
    "#FROM DATAPORT, remove three: AB09, AB05, and AB10\n",
    "#FROM GT, remove six, AB25, AB28, AB30, AB20, AB12, AB09\n",
    "REMOVE_SUBS_XVAL = True\n",
    "\n",
    "if REMOVE_SUBS_XVAL:\n",
    "    #r01 ordered\n",
    "    index_sub_remove_r01 = gait_data_r01_ordered[ (gait_data_r01_ordered['subj_id'] == 2) | (gait_data_r01_ordered['subj_id'] == 6) ].index\n",
    "    gait_data_r01_ordered_val = gait_data_r01_ordered.iloc[index_sub_remove_r01]\n",
    "    gait_data_r01_ordered.drop(index_sub_remove_r01 , inplace=True)\n",
    "    \n",
    "    #r01 randomized\n",
    "    index_sub_remove_r01 = gait_data_r01_randomized[ (gait_data_r01_randomized['subj_id'] == 2) | (gait_data_r01_randomized['subj_id'] == 6) ].index\n",
    "    gait_data_r01_randomized_val = gait_data_r01_randomized.iloc[index_sub_remove_r01]\n",
    "    gait_data_r01_randomized.drop(index_sub_remove_r01 , inplace=True)\n",
    "    \n",
    "    print(gait_data_r01_ordered.head())\n",
    "    print(gait_data_r01_ordered_val.head())\n",
    "    \n",
    "    #dataport ordered\n",
    "    index_sub_remove_dataport = gait_data_dataport_ordered[ (gait_data_dataport_ordered['subj_id'] == 9) | \n",
    "                                                           (gait_data_dataport_ordered['subj_id'] == 5) |\n",
    "                                                          (gait_data_dataport_ordered['subj_id'] == 10)].index\n",
    "\n",
    "    gait_data_dataport_ordered_val = gait_data_dataport_ordered.iloc[index_sub_remove_dataport]\n",
    "    gait_data_dataport_ordered.drop(index_sub_remove_dataport , inplace=True)\n",
    "    \n",
    "    #dataport randomized\n",
    "    index_sub_remove_dataport = gait_data_dataport_randomized[ (gait_data_dataport_randomized['subj_id'] == 9) | \n",
    "                                                           (gait_data_dataport_randomized['subj_id'] == 5) |\n",
    "                                                          (gait_data_dataport_randomized['subj_id'] == 10)].index\n",
    "\n",
    "    gait_data_dataport_randomized_val = gait_data_dataport_randomized.iloc[index_sub_remove_dataport]\n",
    "    gait_data_dataport_randomized.drop(index_sub_remove_dataport , inplace=True)\n",
    "    \n",
    "    #gt ordered\n",
    "    index_sub_remove_gt = gait_data_gt_ordered[ (gait_data_gt_ordered['subj_id'] == 25) | \n",
    "                                               (gait_data_gt_ordered['subj_id'] == 28) |\n",
    "                                              (gait_data_gt_ordered['subj_id'] == 30) |\n",
    "                                              (gait_data_gt_ordered['subj_id'] == 20) |\n",
    "                                              (gait_data_gt_ordered['subj_id'] == 12) |\n",
    "                                              (gait_data_gt_ordered['subj_id'] == 9)].index\n",
    "    \n",
    "    gait_data_gt_ordered_val = gait_data_gt_ordered.iloc[index_sub_remove_gt]\n",
    "    gait_data_gt_ordered.drop(index_sub_remove_gt , inplace=True)\n",
    "    \n",
    "    #gt randomized\n",
    "    index_sub_remove_gt = gait_data_gt_randomized[ (gait_data_gt_randomized['subj_id'] == 25) | \n",
    "                                               (gait_data_gt_randomized['subj_id'] == 28) |\n",
    "                                              (gait_data_gt_randomized['subj_id'] == 30) |\n",
    "                                              (gait_data_gt_randomized['subj_id'] == 20) |\n",
    "                                              (gait_data_gt_randomized['subj_id'] == 12) |\n",
    "                                              (gait_data_gt_randomized['subj_id'] == 9)].index\n",
    "    \n",
    "    gait_data_gt_randomized_val = gait_data_gt_randomized.iloc[index_sub_remove_gt]\n",
    "    gait_data_gt_randomized.drop(index_sub_remove_gt , inplace=True)\n",
    "    \n",
    "    #concatenate\n",
    "    gait_data = pd.concat([gait_data_r01_ordered, gait_data_r01_randomized,\\\n",
    "                          gait_data_dataport_ordered, gait_data_dataport_randomized,\\\n",
    "                          gait_data_gt_ordered, gait_data_gt_randomized])\n",
    "    \n",
    "    gait_data_val = pd.concat([gait_data_r01_ordered_val, gait_data_r01_randomized_val,\\\n",
    "                              gait_data_dataport_ordered_val, gait_data_dataport_randomized_val,\\\n",
    "                              gait_data_gt_ordered_val, gait_data_gt_randomized_val])\n",
    "    \n",
    "    train_dataset = WindowedGaitDataset(gait_data=gait_data,\n",
    "                                                meas_scale=meas_scale,\n",
    "                                                window_size = window_size,\n",
    "                                                speed_scale = speed_scale,\n",
    "                                                incline_scale = incline_scale,\n",
    "                                                stair_height_scale=stair_height_scale,\n",
    "                                                transform=ToTensor())\n",
    "    \n",
    "    val_dataset = WindowedGaitDataset(gait_data=gait_data_val,\n",
    "                                                meas_scale=meas_scale,\n",
    "                                                window_size = window_size,\n",
    "                                                speed_scale = speed_scale,\n",
    "                                                incline_scale = incline_scale,\n",
    "                                                stair_height_scale=stair_height_scale,\n",
    "                                                transform=ToTensor())\n",
    "    \n",
    "    print('{:>5,} training samples'.format(len(train_dataset)))\n",
    "    print('{:>5,} validation samples'.format(len(val_dataset)))\n",
    "\n",
    "\n",
    "else:\n",
    "    gait_data = pd.concat([gait_data_r01_ordered, gait_data_r01_randomized,\\\n",
    "                          gait_data_dataport_ordered, gait_data_dataport_randomized,\\\n",
    "                          gait_data_gt_ordered, gait_data_gt_randomized])\n",
    "    # gait_data = gait_data_r01_randomized\n",
    "    # gait_data = pd.concat([gait_data_r01_randomized, gait_data_dataport_randomized])\n",
    "    \n",
    "    dataset = WindowedGaitDataset(gait_data=gait_data,\n",
    "                                            meas_scale=meas_scale,\n",
    "                                            window_size = window_size,\n",
    "                                            speed_scale = speed_scale,\n",
    "                                            incline_scale = incline_scale,\n",
    "                                            stair_height_scale=stair_height_scale,\n",
    "                                            transform=ToTensor())  \n",
    "    # Create a 90-10 train-validation split.\n",
    "\n",
    "    # Calculate the number of samples to include in each set.\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "    # Divide the dataset by randomly selecting samples.\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    print('{:>5,} training samples'.format(train_size))\n",
    "    print('{:>5,} validation samples'.format(val_size))\n",
    "\n",
    "print(val_dataset[0]['meas'][-1,:])\n",
    "print(val_dataset[1]['meas'][-1,:])\n",
    "print(val_dataset[2]['meas'][-1,:])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Dataloaders to hold the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SET BATCH SIZE\n",
    "BATCH_SIZE_TRAIN = 512\n",
    "BATCH_SIZE_VALIDATE = 2048\n",
    "NUM_WORKERS = 8\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            shuffle=True,\n",
    "            batch_size = BATCH_SIZE_TRAIN, # Trains with this batch size.\n",
    "            pin_memory=True,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )\n",
    "\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "#try shuffle off see if it fixes\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            shuffle=True,\n",
    "            batch_size = BATCH_SIZE_VALIDATE, # Evaluate with this batch size.\n",
    "            pin_memory=True,\n",
    "            num_workers=NUM_WORKERS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some samples for sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "BsKFPRi9dWMW",
    "outputId": "2804459e-debd-45a0-ea50-bbb4250d3dc4"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3,1)\n",
    "\n",
    "for i in range(0,80):\n",
    "  y = train_dataset[i+window_size]['meas'][:,0]\n",
    "  y1 = train_dataset[i+window_size]['meas'][:,4]\n",
    "  x = np.array([j for j in range(len(y))])\n",
    "  x = x + i\n",
    "\n",
    "  yy = train_dataset[i+window_size]['state'][:,0]\n",
    "  axs[0].plot(x,y,linewidth=3)\n",
    "  axs[1].plot(x,y1,linewidth=3)\n",
    "  axs[2].plot(i,yy,'o',linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the model nickname for easy identification betweeen model types. Feel free to choose whatever you want for the nickname. Currently, apollyon-three-stairs denotes the latest gait transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4YR0pTPWV3Dw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "model_nickname = 'apollyon-three-stairs'\n",
    "\n",
    "output_dir = f'../staging_area/{model_nickname}/model_save/'\n",
    "if REMOVE_SUBS_XVAL:\n",
    "    output_dir = f'../staging_area/{model_nickname}/model_save_xval/'\n",
    "\n",
    "# Create output directory if needed\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "checkpoint_dir = 'checkpoints/'\n",
    "if not os.path.exists(output_dir+checkpoint_dir):\n",
    "    os.makedirs(output_dir+checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xJFUThsu9tzX",
    "outputId": "c4a12851-65aa-495b-95ca-c2d18abfe757",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "dim_val = 32 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "n_decoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 7 # The number of input variables. 1 if univariate forecasting.\n",
    "enc_seq_len = 150 # length of input given to encoder. Can have any integer value.\n",
    "dec_seq_len = 1 # length of input given to decoder. Can have any integer value.\n",
    "\n",
    "dropout_encoder = 0.1\n",
    "dropout_decoder = 0.1\n",
    "dropout_pos_enc = 0.0\n",
    "dropout_regression = 0.1\n",
    "dim_feedforward_encoder = 512\n",
    "dim_feedforward_decoder = 512\n",
    "\n",
    "num_predicted_features = 5 # The number of output kinematics. \n",
    "\n",
    "model = GaitTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_heads=n_heads,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dropout_encoder=dropout_encoder,\n",
    "    dropout_decoder=dropout_decoder,\n",
    "    dropout_pos_enc=dropout_pos_enc,\n",
    "    dropout_regression=dropout_regression,\n",
    "    num_predicted_features=num_predicted_features,\n",
    "    dim_feedforward_encoder=dim_feedforward_encoder,\n",
    "    dim_feedforward_decoder=dim_feedforward_decoder,\n",
    ")\n",
    "    \n",
    "enum_parameters(model)\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# set up the class to auto-save the best (lowest-loss) model \n",
    "best_model_name = 'ml_gait_estimator_dec_best_model.tar'\n",
    "save_best_model = SaveBestModel(output_dir+best_model_name)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#set up the optimizer. We use Adam as it's pretty good\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.998), eps=1e-9, weight_decay=1e-4)\n",
    "#set up learning rate scheduler. This scheduler decreases the learning rate if error does not sufficiently decrease\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=0, threshold=0.01, verbose=True,min_lr=1e-6)\n",
    "\n",
    "start_epoch = 0\n",
    "\n",
    "#optionally, load a model from a checkpoint. Useful to pick back up training from a previous point\n",
    "FROM_CHECKPOINT = not True\n",
    "if FROM_CHECKPOINT:\n",
    "    \n",
    "    checkpoint = torch.load(output_dir+'ml_gait_estimator_dec_best_model.tar')\n",
    "    g = checkpoint['model_state_dict']\n",
    "    loss = checkpoint['loss']\n",
    "    print(f'Lowest Loss: {loss}')\n",
    "    save_best_model = SaveBestModel(output_dir+best_model_name, loss)\n",
    "    # print(g.keys())\n",
    "    model.load_state_dict(g)\n",
    "\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    print(f'From epoch: {start_epoch}')\n",
    "\n",
    "\n",
    "#set up the number of epochs to train the model for\n",
    "epochs = 10\n",
    "\n",
    "#set how often to save the model checkpoint\n",
    "SAVE_EVERY_EPOCH_N = 1\n",
    "\n",
    "\n",
    "#define loss function\n",
    "lossfcn = GaitLoss(w_phase=2, w_speed=5, w_incline=10, w_stairs=2)\n",
    "\n",
    "#Set up start of sequence token\n",
    "SOS_token = 100 * torch.ones(1, 1, num_predicted_features).to(device).requires_grad_(False)\n",
    "\n",
    "#Extract index for time steps\n",
    "DT_IDX = 6\n",
    "\n",
    "training_RMSEs = []\n",
    "validation_RMSEs = []\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "\n",
    "for epoch_i in range(start_epoch, start_epoch+epochs):\n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, start_epoch+epochs))\n",
    "    print('Training...')\n",
    "    \n",
    "    #print learning rate, helps to keep track of how far into training we are\n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    print(f'Learning Rate: {lr}')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "    step_ct = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        step_ct += 1\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        \n",
    "        b_input = batch['meas'].to(device) #obtain kinematics inputs to transformer\n",
    "        b_state = batch['state'].to(device) #obtain ground truth gait state\n",
    "                \n",
    "        tgt = SOS_token.repeat(b_state.shape[0], 1, 1) #project the SOS token to a batched input to the transformer decoder\n",
    "\n",
    "        #extract delta time steps used to encode position of kinematics\n",
    "        dts = b_input[:,:,DT_IDX]\n",
    "        dts = torch.unsqueeze(dts, dim=-1)\n",
    "\n",
    "        #RUN THE MODEL ON INPUTS! this is the forward pass\n",
    "        prediction = model(b_input,tgt, dts)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = lossfcn(prediction, b_state)\n",
    "\n",
    "        #backprop loss!\n",
    "        loss.backward()\n",
    "\n",
    "        #optimize parameters using backprop'd loss\n",
    "        optimizer.step()\n",
    "\n",
    "        #obtain value of the loss\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        # Progress update every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            print(loss_value)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss_value\n",
    "        \n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / step_ct           \n",
    "    training_RMSEs.append(avg_train_loss)\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    # Save to checkpoints\n",
    "    if (epoch_i + 1) % SAVE_EVERY_EPOCH_N == 0:\n",
    "        print('Saving model checkpoint')\n",
    "        model_name = f\"ml_gait_estimator_dec_checkpoint_{epoch_i + 1}.tar\"\n",
    "        path_name = output_dir+checkpoint_dir+model_name\n",
    "        print(path_name)\n",
    "        torch.save({\n",
    "                    'epoch':epoch_i + 1,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss.item(),\n",
    "                    }, path_name)\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    step_ct = 0\n",
    "    for batch in validation_dataloader:\n",
    "        step_ct += 1\n",
    "\n",
    "        b_input = batch['meas'].to(device)\n",
    "        b_state = batch['state'].to(device)\n",
    "                \n",
    "        tgt = SOS_token.repeat(b_state.shape[0], 1, 1)\n",
    "        dts = b_input[:,:,DT_IDX]\n",
    "        dts = torch.unsqueeze(dts, dim=-1)\n",
    "\n",
    "        # Don't construct the compute graph during validation forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input, tgt, dts)\n",
    "\n",
    "        loss = lossfcn(outputs, b_state)\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss_value\n",
    "\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / step_ct\n",
    "    \n",
    "    #schedule PlateauLoss on the validation loss\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    validation_RMSEs.append(avg_val_loss)\n",
    "\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    \n",
    "    #save best model, i.e. the model with the lowest validation loss\n",
    "    save_best_model(\n",
    "        avg_val_loss, epoch_i+1, model, optimizer, lossfcn\n",
    "    )\n",
    "    \n",
    "    #print training vals\n",
    "    print('Training vals')\n",
    "    print(validation_RMSEs)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "#save model params\n",
    "model_name = 'ml_gait_estimator_dec_params.pt'\n",
    "torch.save(model.state_dict(), output_dir+model_name)\n",
    "\n",
    "model_name = 'ml_gait_estimator_dec_full.pt'\n",
    "torch.save(model, output_dir+model_name)\n",
    "\n",
    "#save checkpoint\n",
    "model_name = f\"ml_gait_estimator_dec_checkpoint_{epoch_i + 1}.tar\"\n",
    "torch.save({'epoch': epoch_i + 1,\n",
    "                      'model_state_dict': model.state_dict(),\n",
    "                      'optimizer_state_dict': optimizer.state_dict(),\n",
    "                      'loss': loss.item(),\n",
    "                      }, path_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the losses over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "KFrle1f3fxJP",
    "outputId": "65ca3ae9-d33d-4184-bd58-99b6038128f3"
   },
   "outputs": [],
   "source": [
    "training_RMSEs = np.array(training_RMSEs)\n",
    "validation_RMSEs = np.array(validation_RMSEs)\n",
    "fig, axs = plt.subplots()\n",
    "axs.plot(training_RMSEs,'-',label='Train')\n",
    "axs.set_ylabel('Loss (MSE)')\n",
    "axs.plot(validation_RMSEs,'-',label='Val')\n",
    "axs.set_xlabel('Epoch')\n",
    "axs.legend()\n",
    "print(np.min(validation_RMSEs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show model performance, run the model on the entirety of the val data, and compute individual gait state RMSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "9W0fxeTSf7KP",
    "outputId": "1850af91-1efc-41db-abde-d713f9e16397",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Test model\n",
    "test_dataset = val_dataset\n",
    "\n",
    "BATCH_SIZE = 1024*4\n",
    "prediction_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False,num_workers=8)\n",
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test points...'.format(len(test_dataset)))\n",
    "\n",
    "# Model parameters\n",
    "dim_val = 32 # This can be any value divisible by n_heads. 512 is used in the original transformer paper.\n",
    "n_heads = 4 # The number of attention heads (aka parallel attention layers). dim_val must be divisible by this number\n",
    "n_encoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "n_decoder_layers = 4 # Number of times the encoder layer is stacked in the encoder\n",
    "input_size = 7 # The number of input variables. 1 if univariate forecasting.\n",
    "enc_seq_len = 150 # length of input given to encoder. Can have any integer value.\n",
    "dec_seq_len = 1 # length of input given to decoder. Can have any integer value.\n",
    "\n",
    "dropout_encoder = 0.1\n",
    "dropout_decoder = 0.1\n",
    "dropout_pos_enc = 0.0\n",
    "dropout_regression = 0.1\n",
    "dim_feedforward_encoder = 512\n",
    "dim_feedforward_decoder = 512\n",
    "\n",
    "num_predicted_features = 5 # The number of output variables. \n",
    "\n",
    "best_model = GaitTransformer(\n",
    "    dim_val=dim_val,\n",
    "    input_size=input_size, \n",
    "    n_encoder_layers=n_encoder_layers,\n",
    "    n_decoder_layers=n_decoder_layers,\n",
    "    n_heads=n_heads,\n",
    "    enc_seq_len=enc_seq_len,\n",
    "    dropout_encoder=dropout_encoder,\n",
    "    dropout_decoder=dropout_decoder,\n",
    "    dropout_pos_enc=dropout_pos_enc,\n",
    "    dropout_regression=dropout_regression,\n",
    "    num_predicted_features=num_predicted_features,\n",
    "    dim_feedforward_encoder=dim_feedforward_encoder,\n",
    "    dim_feedforward_decoder=dim_feedforward_decoder,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU.\")\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "best_model.to(device)\n",
    "\n",
    "model_nickname = 'apollyon-three-stairs'\n",
    "\n",
    "model_dir = f'../staging_area/{model_nickname}/model_save/'\n",
    "if REMOVE_SUBS_XVAL:\n",
    "    model_dir = f'../staging_area/{model_nickname}/model_save_xval/'\n",
    "    \n",
    "\n",
    "checkpoint = torch.load(model_dir+'ml_gait_estimator_dec_best_model.tar')\n",
    "g = checkpoint['model_state_dict']\n",
    "loss = checkpoint['loss']\n",
    "print(f'Lowest Loss: {loss}')\n",
    "best_model.load_state_dict(g)\n",
    "\n",
    "\n",
    "epoch = checkpoint['epoch']\n",
    "\n",
    "# Put model in evaluation mode\n",
    "best_model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions_raw, true_labels_raw = [], []\n",
    "\n",
    "#Set up start of sequence token\n",
    "SOS_token = 100 * torch.ones(1, 1, num_predicted_features).to(device).requires_grad_(False)\n",
    "\n",
    "#Extract index for time steps\n",
    "DT_IDX = 6\n",
    "\n",
    "\n",
    "# Predict \n",
    "print(len(prediction_dataloader))\n",
    "total_t0 = time.time()\n",
    "\n",
    "for batch in prediction_dataloader:\n",
    "\n",
    "    b_input = batch['meas'].to(device)\n",
    "    b_state = batch['state'].to(device)\n",
    "    \n",
    "    tgt = SOS_token.repeat(b_state.shape[0], 1, 1)\n",
    "    dts = b_input[:,:,DT_IDX]\n",
    "    dts = torch.unsqueeze(dts, dim=-1)\n",
    "\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      outputs = best_model(b_input,tgt, dts)\n",
    "\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    outputs = outputs.detach().to('cpu').numpy()\n",
    "    b_state = b_state.to('cpu').numpy()\n",
    "  \n",
    "    # Store predictions and true labels\n",
    "    outputs = np.squeeze(outputs, axis=1)\n",
    "    b_state = np.squeeze(b_state, axis=1)\n",
    "    \n",
    "    # print(outputs.shape)\n",
    "    \n",
    "    #unscale\n",
    "    outputs = unscale_gait_state(outputs, speed_scale, incline_scale, stair_height_scale)\n",
    "    b_state = unscale_gait_state(b_state, speed_scale, incline_scale, stair_height_scale)\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    predictions_raw.extend(outputs.tolist())\n",
    "    true_labels_raw.extend(b_state.tolist())\n",
    "\n",
    "print('    DONE PREDICTING')\n",
    "print(\"Total predicting took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute gait state RMSEs and plot the predicted vs actual gait states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array(predictions_raw)\n",
    "true_labels = np.array(true_labels_raw)\n",
    "\n",
    "#round stairs to three locomotion modes\n",
    "true_labels[true_labels[:,3] < -0.5,3] = -1\n",
    "true_labels[np.logical_and(true_labels[:,3] >= -0.5, true_labels[:,3] <= 0.5),3] = 0\n",
    "true_labels[true_labels[:,3] > 0.5,3] = 1\n",
    "\n",
    "predictions[predictions[:,3] < -0.5,3] = -1\n",
    "predictions[np.logical_and(predictions[:,3] >= -0.5, predictions[:,3] <= 0.5),3] = 0\n",
    "predictions[predictions[:,3] > 0.5,3] = 1\n",
    "\n",
    "\n",
    "phase_losses = np.sqrt(phase_dist(predictions[:,0], true_labels[:,0])**2)\n",
    "speed_losses = np.sqrt((predictions[:,1] - true_labels[:,1])**2)\n",
    "incline_losses = np.sqrt((predictions[:,2] - true_labels[:,2])**2)\n",
    "\n",
    "stair_height_accuracy = np.sum(true_labels[:,3] == predictions[:,3])/len(true_labels[:,3])\n",
    "stair_height_accuracy_ascent = np.sum(true_labels[true_labels[:,3] == 1,3] == predictions[true_labels[:,3] == 1,3])/len(true_labels[true_labels[:,3] == 1,3])\n",
    "stair_height_accuracy_descent = np.sum(true_labels[true_labels[:,3] == -1,3] == predictions[true_labels[:,3] == -1,3])/len(true_labels[true_labels[:,3] == -1,3])\n",
    "\n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "print(\"=\"*30)\n",
    "print(f'Phase Losses: {np.mean(phase_losses):.3f} +- {np.std(phase_losses):.3f}')\n",
    "print(f'Speed Losses: {np.mean(speed_losses):.3f} +- {np.std(speed_losses):.3f}')\n",
    "print(f'Incline Losses: {np.mean(incline_losses):.3f} +- {np.std(incline_losses):.3f}')\n",
    "# print(f'Stair Height Losses: {np.mean(stair_height_losses):.3f} +- {np.std(stair_height_losses):.3f}')\n",
    "# print(f'Stair Height Losses On Stairs: {np.mean(stair_height_losses_on_stairs):.3f} +- {np.std(stair_height_losses_on_stairs):.3f}')\n",
    "\n",
    "print(f'Is Stairs Accuracy: {stair_height_accuracy:.3f}')\n",
    "print(f'Is Stairs Accuracy, Ascent: {stair_height_accuracy_ascent:.3f}')\n",
    "print(f'Is Stairs Accuracy, Descent: {stair_height_accuracy_descent:.3f}')\n",
    "\n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4,1,figsize=(20,8),sharex=True)\n",
    "axs[0].plot(predictions[:,0],'r',label='predict')\n",
    "axs[0].plot(true_labels[:,0],'b',label='actual')\n",
    "axs[0].legend()\n",
    "# axs[0].set_xlim([9000,20000])\n",
    "# axs[0].set_xlim([0.5e6,0.5e6+5000])\n",
    "# axs[0].set_xlim([80,250000]) #ordered r01\n",
    "# axs[0].set_xlim([1.47e5,1.75e5]) #ordered r01 stairs\n",
    "\n",
    "# axs[0].set_xlim([80+1000,0.05e6+1000])\n",
    "# axs[0].set_xlim([0.3622e6,0.3725e6]) #r01 randomized mix of stairs and inclines\n",
    "# axs[0].set_xlim([0.25e6,0.26e6])\n",
    "# axs[0].set_xlim([2e6,2.02e6]) #dataport mix of ramps\n",
    "# axs[0].set_xlim([0.15e6,0.16e6])\n",
    "axs[0].set_xlim([3.175e6,3.19e6]) #ordered gt\n",
    "# axs[0].set_xlim([3.5e6,3.53e6]) #randomized gt\n",
    "\n",
    "axs[0].set_ylabel('Phase')\n",
    "\n",
    "axs[1].plot(predictions[:,1],'r',label='predict')\n",
    "axs[1].plot(true_labels[:,1],'b',label='actual')\n",
    "axs[1].set_ylabel('Speed (m/s)')\n",
    "axs[1].set_ylim([-1.25,2])\n",
    "\n",
    "\n",
    "axs[2].plot(predictions[:,2],'r',label='predict')\n",
    "axs[2].plot(true_labels[:,2],'b',label='actual')\n",
    "axs[2].set_ylabel('Incline (deg)')\n",
    "# axs[2].set_ylim([-15,15])\n",
    "\n",
    "axs[3].plot(predictions[:,3],'r.',label='predict')\n",
    "axs[3].plot(true_labels[:,3],'b',label='actual')\n",
    "axs[3].set_ylabel('Is Stairs')\n",
    "# axs[3].set_ylim([-0.2,1.2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in Vertex AI: copy the models from the preset staging_area folder (which isn't tracked via GitHub) to the tracked full_models folder to save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7PxtSeLVlak"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../staging_area/apollyon-three-stairs/ ../full_models/\n",
    "!zip -r ../full_models/apollyon-three-stairs.zip ../full_models/apollyon-three-stairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For use in Vertex AI: copy the models from the tracked full_models folder to a dedicated bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7184MNpqVlal"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r ../full_models/apollyon-three-stairs/ gs://ml_gait_estimation/full_models/\n",
    "!gsutil cp -r ../full_models/apollyon-three-stairs.zip gs://ml_gait_estimation/full_models/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-13.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m107"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
